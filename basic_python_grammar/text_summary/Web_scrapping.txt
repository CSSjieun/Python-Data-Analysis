프로그램으로 웹사이트의 페이지를 옮겨 가면서 데이터를 추출하는 작업을 web scrapping 혹은 web crawling이라고 부릅니다. 

loc 메서드는 인덱스와 열 이름을 사용합니다. 열 이름이 곧 인덱스 이기 때문에 열 인덱스라고도 부릅니다.
(예) books.loc[[0,1], ['column1', 'column2']]

iloc 메서드는 인덱스의 위치를 사용합니다.
books의 행 인덱스는 0부터 시작하므로 인덱스와 인덱스의 위치가 같습니다.
열의 경우 'no'부터 위치가 0에서 시작하여 1씩 증가합니다.
따라서 앞의 코드를 iloc로 다시 쓰면 다음과 같습니다: books.iloc[[0,1], [1,2]]

파이썬 프로그래머들은 웹 페이지나 웹 기반 API를 호출하는데 requests 패키지를 많이 사용합니다.
비슷하게 HTML 안에 있는 내용을 찾을 때는 Beautiful Soup가 널리 사용됩니다.
Scrapy 역시 많이 사용되는 패키지 입니다. 

soup = BeautifulSoup(r.text, 'html.parser')
첫번째 매개변수는 parsing할 때 HTML 문서이고 두 번째는 parsing에 사용할 parser입니다.

Paser는 입력 데이터를 받아 데이터 구조를 만드는 소프트웨어 라이브러리를 의미합니다.
그리고 이러한 과정을 parsing이라고 합니다.
json, xml 패키지가 각각 JSON과 XML을 위한 paser라고 볼 수 있습니다.

# 태그 위치 찾기
태크 위치는 soup 객체의 find() 메서드를 사용하면 간편하게 찾을 수 있습니다.
첫 번째 매게변수에는 찾을 태그 이름을 지정하고, attrs 매개변수에는 찾으려는 태그의 속성을 딕셔너리로 지정하면 됩니다.
예를 들어 soup.find('div', attrs={'id':'search'})는 id 속성이 search인 <div> 태그를 찾으라는 의미입니다. 

# tag 안의 텍스트 가져오기 get_text()
<tr> tag를 리스트로 추출하고 나면 다음 작업은 간단합니다.
for 문으로 prd_tr_list를 순회하면서 <th> 태크 안의 텍스트가 '쪽수, 무게, 크기'에 해당하는지 검사합니다.
우리가 원하는 행을 찾으면 <td> 태그 안에 담긴 텍스트를 page_td 변수에 저장하면 됩니다.

Yes24 website에서 도서 상세 페이지 HTML을 가져온 후 품목정보가 들어 있는 <div> 태그를 찾았습니다.
그다음 이 태그 안에 있는 테이블에서 쪽수가 들어 있는 텍스트를 가져왔습니다.
첫 번째 도서의 쪽수를 얻는데 성공했으므로 나머지 도서들의 쪽수도 동일한 방법으로 가져올 수 있습니다.

* 온라인 서점 사이트의 도서 상세 페이지는 웹 페이지에 포함된 이벤트나 광고 등으로 책마다 조금씩 다르게 구성됩니다. 
따라서 항상 동일한 HTML 태그를 추출하는 방법을 찾으려면 여러 페이지에서 테스트하고 시행착오를 거쳐야 합니다. 

# 전체 도서의 쪽수 구하기
앞에 했던 작업을 하나의 함수로 만들어보겠습니다.
(1) 온라인 서점의 검색 결과 페이지 URL을 만듭니다.
(2) requests.get() 함수로 검색 결과 페이지의 HTML을 가져옵니다.
(3) 뷰티플수프로 HTML을 파싱합니다.
(4) 뷰티플수프의 find() 메서드로 <a> 태그를 찾아 상세 페이지 URL을 추출합니다.
(5) requests.get() 함수로 다시 도서 상세 페이지의 HTML을 가져옵니다.
(6) 뷰티플수프로 HTML을 파싱합니다.
(7) 뷰티플수프의 find() 메서드로 '품목정보' <div> 태그를 찾습니다.
(8) 뷰티플수프의 find_all() 메서드로 '쪽수'가 들어있는 <tr> 태그를 찾습니다.
(9) 앞에서 찾은 테이블의 행에서 get_text() 메서드로 <td> 태그에 들어있는 '쪽수'를 가져옵니다.

# lambda함수
lambda 함수는 함수 이름 없이 한 줄로 쓰는 함수입니다.
코드를 간결하게 작성할 수 있어 함수를 간단하고 빠르게 구현할 수 있죠.

본문에서 작성한 get_page_cnt2() 함수 대신 람다 함수를 apply() 메서드에 사용하려면 다음과 같이 사용할 수 있습니다.

page_count = top10_books.apply(lambda row: get_page_cnt(row['ISBN']), axis=1)

람다 함수로 코드를 바꾸어서 실행해 보세요.
훨씬 간단하게 작성하고도 동일한 결과를 얻을 수 있습니다.
어떤 방식을 사용할지는 작성자에 따라서 다릅니다.
처음에는 람다 함수 작성이 쉽지 않을 수 있지만, apply() 메서드에 행 또는 열을 전달하는지 의도를 분명하게 드러낼 수 있는 장점이 있습니다.

또 다른 예를 들면 some_df라는 데이터 프레임의 각 열에 apply() 메서드를 적용하고 싶다면 다음과 같이 쓸 수 있습니다.

some_df.apply(lambda colum: some_fun(column), axis=0)

# Web Scrapping 할 때 주의할 점
HTTP 요청이 많으면 웹 서버에 부담되므로 웹 사이트 입장에서는 달가운 손님은 아닙니다.
어떤 웹사이트는 한 컴퓨터에서 짧은 시간 동안 맣은 양의 요청을 발생시키는 경우 일정 기간동안 접속하지 못하게 막기도 하므로 다음 사항을 주의하세요.

1. 웹 사이트에서 스크래핑을 허락하였는지 확인하세요.
대부분의 웹 사이트는 검색 엔진이나 스크래핑 프로그램이 접근해도 좋은 페이지와 그렇지 않은 페이지를 명시한 robots.txt 파일을 가지고 있습니다.
예를 들어 Yes24의 robots.txt 파일을 확인해보면 다행히 검색 결과 페이지인 /Product/Search와 도서 상세 페이지인 /Product/Goods는 따로 기재되어 있지 않으므로 제한하지 않는다는 의미로 볼 수 있습니다.
대신 /member/, /Product/Goods/addModules/ 등과 같은 페이지는 웹 스크래핑 도구로 접근해서는 안됩니다.

2. HTML 태그를 특정할 수 있는지 확인하세요.
tag 이름이나 속성 등 필요한 HTML 태그를 특정할 수 없다면 웹 스크래핑으로 데이터를 가져오는데 어려움이 있습니다.
또한 일부 웹 페이지는 웹 브라우저에 로딩된 후 HTML 대신 Javascript를 사용하여 웹 서버로부터 데이터를 가져와 화면을 채웁니다.
이런 데이터는 단순한 웹 스크래핑을 사용해서 가져오기 힘들 수 있습니다.
Selenijm 같은 고급 도구를 사용해야 가능합니다.

API나 데이터베이스와 달리 웹 페이지는 언제 어떻게 바귈지 모르기 때문에 스크래핑 프로그램이 원하는 데이터를 찾지 못하는 경우가 빈번하게 발생합니다.
웹 페이지가 변경된 것을 알았더라도 HTML을 다시 분석하여 원하는 데이터를 찾는 과정을 되풀이 해야 합니다.
이는 스크래핑 프로그램의 유지 보수를 어렵게 만드는 이유 중 하나 입니다.

# Web Scrapping으로 HTML 수집하기
Python의 requests 패키지와 BeautifulSoup를 사용하면 비교적 간단하게 웹 페이지의 HTML에서 원하는 요소를 추출할 수 있습니다.
웹 브라우저의 개발자 도구로 원하는 정보가 들어가 있는 HTML 태그를 확인했습니다.
그리고 찾은 HTML 태그이ㅡ 이름과 속성을 활용해 뷰티플수프 객체로 원하는 정보를 골라내었습니다.

requests 패키지와 BeautifulSoup로 첫 번째 도서의 쪽수를 가져오는 데 성공한 후 단계별로 수행했던 작업을 하나의 파이썬 함수로 만들었습니다.
ISBN을 입력받고 쪽수를 반환하며 세부적인 웹 스크래핑 작업을 감싸는 함수입니다.
데이터 프레임에 있는 모든 도서의 쪽수를 가져오기 위해 만든 함수와 판다스 데이터프레임의 apply() 메서드를 함께 사용했습니다.
그 다음 apply() 메서드가 반환한 판다스 시리즈 객체를 원래 데이터프레임과 함치기 위해 merge() 메서드를 사용했습니다.

# 웹 스크래핑은 웹사이트에서 필요한 데이터를 추출하는 기술입니다.
HTML은 구조적이지 않기 때문에 스크래핑으로 데이터를 수집하는데 비교적 많은 노력이 필요합니다.
따라서 웹 스크래핑을 사용하기 전에 먼저 공개 API를 통해 사용할 수 있는지 살펴보는 것이 좋습니다.

# BeautifulSoup는 HTML 문서를 parsing하는 데 사용하는 대표적인 Python package입니다. 사용법이 쉽고 빠르기 때문에 python programmer들이 많이 사용합니다.
BequtifulSoup는 requests package로 가져온 HTML에서 원하는 태그나 텍스트를 찾는 기능을 제공합니다.

loc - 레이블(이름) 또는 불리언 배열로 데이터 프레임의 행과 열을 선택합니다. 정수로 지정하면 인데스의 레이블로 간주하고 불리언 배열로 지정하면 배열의 길이는 행 또는 열의 전체 길이와 같아야 합니다.
BeautifulSoup.find() - 현재 태그 아래의 자식 태그 중에서 지정된 이름에 맞는 첫 번째 태그를 찾습니다.
BeautifulSoup.find_all() - 현재 태그 아래의 자식 태그 중에서 지정된 이름에 맞는 모든 태그를 찾습니다.
BeautifulSoup.get_text() - 태그 안의 텍스트를 반환합니다.
DataFrame.apply() - 데이터프레임의 행 또는 열에 지정한 함수를 적용합니다.
pandas.merge() - 데이터프레임이나 시리즈 객체를 합칩니다.




